{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029d1b74-7ae0-4fbe-b120-2b77e5fc04b7",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df0827-9cc4-4f40-b584-2dc6042a14cc",
   "metadata": {},
   "source": [
    "We import the necessary libraries for web scraping and data handling. The `CHECK_DATE` stores the current date for labeling output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f038e0-9874-42fe-a970-e7e6d4a14a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "CHECK_DATE = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307ddd8-ccf0-4a06-93c5-2958eaa72389",
   "metadata": {},
   "source": [
    "### 2. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b981c15-b0a6-4b7e-99eb-0a2bb7389129",
   "metadata": {},
   "source": [
    "To extract product data from Co.op, we follow a multi-step process: fetch product categories, fetch product codes from category pages, collect item data by category, clean the results, and save everything in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c87fa-e3f6-47b7-bc5f-113ed24e292f",
   "metadata": {},
   "source": [
    "#### 2.1. Fetching Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aeb07b-5a7b-4292-9dc6-dda7f17a0240",
   "metadata": {},
   "source": [
    "We start by extracting product categories from a saved HTML page that represents the structure of the Co.op Online website. The script navigates through three levels of category hierarchy: top-level categories, their subcategories, and third-level subsubcategories.\n",
    "\n",
    "Each category is stored as a dictionary containing the category *name*, *level* (1 for top-level, 2 for subcategory, 3 for subsubcategory), a flag indicating whether it has children, its *parent* category (if any), and the *link* associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f80aa-d4d8-4000-a7ec-c89adc81c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"groups.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    groups_page = BeautifulSoup(file, \"html.parser\")    # take the html page saved before and put it into a BeautifulSoup object\n",
    "\n",
    "categories = []\n",
    "top_categories = groups_page.find_all(\"a\", class_=\"clearfix\", href=re.compile(r\"https://cooponline.vn/groups/[^#]\"))    # find all top-level categories\n",
    "\n",
    "# loop through all categories and extract relevant fields\n",
    "for category in top_categories:\n",
    "    \n",
    "    categories.append({                  # extract only relevant data and save into a dictionary\n",
    "        \"name\": category.span.string, # category name for reference\n",
    "        \"level\": 1, # hierarchy level for reference\n",
    "        \"hasChild\": True, # hardcoded True - top-level categories has children, important for the products fetching\n",
    "        \"parent\": None, # parent category for reference\n",
    "        \"link\": category[\"href\"] # link to a category page, important for getting product codes further\n",
    "    })\n",
    "    \n",
    "    submenu = category.find_next_sibling(\"div\", class_=\"sub-menu\")    # the subcategories are contained in the following div block\n",
    "    subcategories = submenu.find_all(\"a\", class_=\"main-menu\")    # find all subcategories and loop through them too\n",
    "    for subcategory in subcategories:\n",
    "\n",
    "        subsubmenu = subcategory.find_next_sibling(\"ul\")    # check if the subcategory has child categories\n",
    "        \n",
    "        categories.append({    # add subcategories to the list of dictionaries too\n",
    "            \"name\": subcategory.string,\n",
    "            \"level\": 2,\n",
    "            \"hasChild\": True if subsubmenu else False,   # subsubmenu can be either None or a list of third-level categories\n",
    "            \"parent\": category.span.string,\n",
    "            \"link\": subcategory[\"href\"]\n",
    "        })\n",
    "\n",
    "        # if subsubcategories exists, loop through them and extract too\n",
    "        if subsubmenu: \n",
    "            \n",
    "            subsubcategories = subsubmenu.find_all(\"a\")\n",
    "            for subsubcategory in subsubcategories:\n",
    "                \n",
    "                categories.append({\n",
    "                    \"name\": subsubcategory.string,\n",
    "                    \"level\": 3,\n",
    "                    \"hasChild\": False,\n",
    "                    \"parent\": subcategory.string,\n",
    "                    \"link\": subsubcategory[\"href\"]\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ad024-8c2a-4312-af6b-4e08da7dcec7",
   "metadata": {},
   "source": [
    "After building the list, we clean up the category names by stripping leading and trailing spaces to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6df5b-f553-4cfa-9a19-beacfe30e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    category[\"name\"] = category[\"name\"].strip()\n",
    "    if category[\"parent\"] is not None:\n",
    "        category[\"parent\"] = category[\"parent\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257d06a-1c5d-4425-908f-ccea1f471f60",
   "metadata": {},
   "source": [
    "#### 2.2. Fetching Product Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb478a9b-c4de-402c-aa76-f1e8b28ab879",
   "metadata": {},
   "source": [
    "Unlike the other websites in this project, Co.op Online requires product codes to be explicitly specified in the POST request when fetching product data. These codes are embedded within the HTML of each category’s webpage. To proceed, we need to visit each category page and extract the relevant codes.\n",
    "\n",
    "While all categories contain products, we focus on the most specific ones—those without any child categories. These are smaller and more targeted, which helps reduce the risk of timeouts or rate-limiting during batch data requests. On each category page, we locate the `module-taxonomy` tag, which holds the category’s `term_id` and a list of product codes within its `items` attribute. This information is saved along with the category metadata for use in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e882ce6-af66-4fe5-a433-1d184c428fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the file with headers first\n",
    "categories_df = pd.DataFrame(columns=[\"name\", \"level\", \"hasChild\", \"parent\", \"link\", \"term_id\", \"item_codes\"])\n",
    "categories_df.to_csv(f'categories-{CHECK_DATE}-coop.csv', index=False, mode='w')\n",
    "\n",
    "total_categories_count = len(categories)    # number of categories that we will scan, for progress indication\n",
    "fetched_categories_count = 0 # counter for fetching progress tracker\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    print(f'{category[\"name\"]}..', end='') # current category indication\n",
    "    \n",
    "    if not category[\"hasChild\"]: # if category has no children, we get into it\n",
    "        \n",
    "        print('has no children, looking for codes', end='') # progress indication\n",
    "\n",
    "        # fetch the HTML of a category page, locate term_id and item_codes and save them\n",
    "        current_page = requests.get(category[\"link\"]).text\n",
    "        page_bs = BeautifulSoup(current_page, \"html.parser\")\n",
    "        products_tag = page_bs.find(\"module-taxonomy\")\n",
    "        if products_tag is not None:\n",
    "            category[\"term_id\"] = products_tag[\"term_id\"]\n",
    "            category[\"item_codes\"] = products_tag[\"items\"]\n",
    "\n",
    "        # save into the DataFrame and append to the CSV\n",
    "        categories_df = pd.DataFrame([category])\n",
    "        categories_df.to_csv(f'categories-{CHECK_DATE}-coop.csv', index=False, mode='a', header=False)\n",
    "    else: # if category has child categories, we don't parse HTML, but still add it to the DataFrame and CSV\n",
    "        print('has children, skip', end='')\n",
    "        category[\"term_id\"] = None\n",
    "        category[\"item_codes\"] = None\n",
    "        categories_df = pd.DataFrame([category])\n",
    "        categories_df.to_csv(f'categories-{CHECK_DATE}-coop.csv', index=False, mode='a', header=False)\n",
    "        \n",
    "    fetched_categories_count += 1\n",
    "    print(f'..fetched - {fetched_categories_count} out of {total_categories_count}')\n",
    "    \n",
    "    time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297d177-3fcf-4408-b13d-88bb67797e3d",
   "metadata": {},
   "source": [
    "We load a resulting category reference table and convert missing values to `None` to ensure compatibility with later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fabf06-1ba1-49eb-b2b9-b0cf4ebde184",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv('categories-2025-03-06-coop-complete.csv', dtype={\"term_id\": str})\n",
    "categories = categories.where(pd.notna(categories), None)  # convert NaN to None\n",
    "categories = categories.to_dict('records')\n",
    "# categories = categories[:] # this line can be used to resume scraping from a specific point in case of interruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf842aa2-f675-48fa-9242-c43d1978db5a",
   "metadata": {},
   "source": [
    "#### 2.3. Fetching Raw Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571c725-4169-46d1-bd8e-d84626af6e39",
   "metadata": {},
   "source": [
    "To retrieve product listings from Co.op Online, we need to replicate the website’s behavior when users browse a category. Unlike some other sites where product data is retrieved based on category IDs or slugs, Co.op requires a list of product codes to be included in each POST request.\n",
    "\n",
    "We iterate through each specific (non-parent) category and use the `term_id` and product codes we previously extracted to send POST requests to the site’s backend. Each request fetches a batch of up to 24 products. Pagination is handled using an increasing page number (`trang`), and we continue making requests until the returned batch contains fewer than 24 products, signaling the end of available items for that category.\n",
    "\n",
    "To avoid detection or rate-limiting, a random delay is introduced between requests. The final result is a list of raw product dictionaries in the site’s internal JSON format, ready for further cleaning and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2626d63-d09d-419b-bbb9-0358c5650978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMS_HEADERS = {\n",
    "    'origin': 'https://cooponline.vn',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff0bf8-4b3d-4911-9e12-59047912b228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_products(category):\n",
    "    \n",
    "    current_products = 24 # set to 24 in order to have at least 1 iteration of while loop\n",
    "    current_page = 1\n",
    "    products = []\n",
    "\n",
    "    # set the referer header to match the category being requested\n",
    "    ITEMS_HEADERS['referer'] = category[\"link\"]\n",
    "    url = 'https://cooponline.vn/ajax/'\n",
    "        \n",
    "    while current_products >= 24: # loop until current page contains less than 24 products\n",
    "        \n",
    "        time.sleep(random.uniform(1, 5)) # random delay to avoid rate-limiting or blocking\n",
    "\n",
    "        # construct request body for POST request\n",
    "        DATA = {\n",
    "            'request': 'w_getProductsTaxonomy',\n",
    "            'termid': category[\"term_id\"],\n",
    "            'taxonomy': 'groups',\n",
    "            'store': 'xtanphong',\n",
    "            'items': category[\"item_codes\"],\n",
    "            'trang': current_page,\n",
    "        }\n",
    "    \n",
    "        response = requests.post('https://cooponline.vn/ajax/', headers=ITEMS_HEADERS, data=DATA) # fetch product data for the current category and page\n",
    "        response_data = json.loads(response.text) # convert JSON response to a list of category dictionaries\n",
    "        products += response_data # extract products and add them to the list\n",
    "        \n",
    "        current_products = len(response_data) # update current number of products\n",
    "        current_page += 1 # update page number\n",
    "        \n",
    "        print(f'{len(products)}..', end='') # progress indication\n",
    "\n",
    "    return products "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc9594-42a9-4e3d-b474-3c181d5634b9",
   "metadata": {},
   "source": [
    "#### 2.4. Cleaning Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db589ac-c31c-4ccf-b79e-b806008b596b",
   "metadata": {},
   "source": [
    "Once the raw product listings are collected, we extract only the essential information for analysis. This includes:\n",
    "\n",
    "- Category name (for context)\n",
    "- Product name\n",
    "- Price\n",
    "- Unit of measurement\n",
    "- Supermarket name (hardcoded for clarity)\n",
    "\n",
    "This step reduces the raw JSON response to a clean, uniform structure that can easily be stored, aggregated, or visualized. The output is a list of dictionaries, each representing a simplified product entry, ready for export or merging with data from other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e9901-c591-41da-938b-3cf9f5a76030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_data(category, raw_products):\n",
    "\n",
    "    cleaned_products = []\n",
    "\n",
    "    # go through the raw data and select only the necessary fields\n",
    "    for product in raw_products:\n",
    "        cleaned_products.append({\n",
    "            \"category_name\": category[\"name\"],\n",
    "            \"name\": product[\"name\"],\n",
    "            \"price\": product[\"price\"],\n",
    "            \"uom\": product[\"unit\"],\n",
    "            \"supermarket\": \"Co.op\"\n",
    "        })\n",
    "\n",
    "    return cleaned_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f81a6f-1d70-4f65-af48-f172ab5f0274",
   "metadata": {},
   "source": [
    "#### 2.5. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cfd69-5e44-47e2-9b8a-fecc03d60e68",
   "metadata": {},
   "source": [
    "The following code:\n",
    "\n",
    "- Iterates through the selected categories containing product codes\n",
    "- Fetches and cleans product data for each\n",
    "- Appends the cleaned data to a single CSV file to build a complete dataset\n",
    "\n",
    "A progress tracker prints a summary after each category to monitor scraping progress. A random delay is added between requests to reduce the risk of being blocked. The output filename includes a timestamp (via `CHECK_DATE`) for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fd336-2477-43a0-9b3d-7e0cde4bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_categories_count = 0    # counter for fetching progress tracker\n",
    "total_categories_count = len(categories)\n",
    "\n",
    "# create the file with headers first\n",
    "products_df = pd.DataFrame(columns=[\"category_name\", \"name\", \"price\", \"uom\", \"supermarket\"])\n",
    "products_df.to_csv(f'scraped_products-{CHECK_DATE}-coop.csv', index=False, mode='w')\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    if category[\"item_codes\"] is not None: # check that the category isn't empty\n",
    "        \n",
    "        raw_products = fetch_products(category) # fetch products\n",
    "        new_products = clean_product_data(category, raw_products) # select only relevant data and add new products to the list\n",
    "    \n",
    "        products_df = pd.DataFrame(new_products)\n",
    "        products_df.to_csv(f'scraped_products-{CHECK_DATE}-coop.csv', index=False, mode='a', header=False)\n",
    "\n",
    "        print(f'Category \"{category[\"name\"]}\" finished..', end='')\n",
    "    \n",
    "    else:\n",
    "        print(f'Skip category \"{category[\"name\"]}\"..', end='')    \n",
    "    \n",
    "    fetched_categories_count += 1\n",
    "    print(f'{fetched_categories_count} out of {total_categories_count} categories fetched')\n",
    "    \n",
    "    time.sleep(random.uniform(1, 5))\n",
    "\n",
    "print(f'Fetching complete. Results saved to scraped_products-{CHECK_DATE}-coop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3de000-d6ad-4fad-b7d0-73db62428d00",
   "metadata": {},
   "source": [
    "### 3. Filtering and Normalizing Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b91537-a46c-4cb6-bbfa-f90cf79b0db5",
   "metadata": {},
   "source": [
    "After collecting and cleaning the raw product data, we proceed with filtering the dataset to include only the products relevant for comparison. This step involves several stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac9b41-70d2-4b49-a851-c51ec1e7502c",
   "metadata": {},
   "source": [
    "#### 3.1. Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a587403-0175-41eb-89db-eedbda38f337",
   "metadata": {},
   "source": [
    "We begin by loading the previously saved product and category datasets.\n",
    "\n",
    "- The `category_name` column is dropped, as it is no longer needed for the next steps.\n",
    "- Duplicate product entries are removed — these may occur if a product was listed under multiple categories.\n",
    "- Extra spaces in product names are stripped to ensure consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbb8a0-7e8e-4fc1-83c0-e53cd4ba0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "categories = pd.read_csv('categories-2025-03-06-coop-complete.csv')\n",
    "products_original = pd.read_csv('scraped_products-2025-03-07-coop-complete.csv')\n",
    "\n",
    "products = products_original.drop(['category_name'], axis=1)    # drop category_name column\n",
    "products = products.drop_duplicates() # remove duplicates\n",
    "products.loc[:,'name'] = products['name'].str.strip() # strip extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6873408-f38b-44f2-8be9-576339f729a0",
   "metadata": {},
   "source": [
    "#### 3.2. Filtering Products by Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d19bf8-279b-4812-a1c1-de755a36c0bd",
   "metadata": {},
   "source": [
    "To identify relevant products for comparison, we define a dictionary mapping product types to regular expressions.\n",
    "Each expression captures the base form of the product while deliberately excluding variations (e.g., flavored, processed, or pickled) that fall outside the scope of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2c9c2-2bff-45e8-b7a3-dac81a527e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_regex_map = {\n",
    "    'rice': r'^gạo(?!.*(lứt|lức|dưỡng|nếp))',\n",
    "    'bread': r'^bánh (mì|mỳ|sandw|bag)(?!.*(bông|thịt|bơ|kem|hoa cúc|gà|pate|xốt|sữa|floss|socola|khoai|trứng|trong|hươu|nho|smile))',\n",
    "    'chicken_fillet': r'(file|phi lê|\\bức)(?!.*đùi).*gà',\n",
    "    'pork_leg': r'đùi.*heo',\n",
    "    'egg': r'^trứng gà(?!.*(ăn liền|tiềm|nướng|cay))', # matches \"trứng gà\", but excludes \"trứng vịt\" and varieties like already cooked eggs\n",
    "    'cucumber': r'^dưa.*leo',\n",
    "    'carrot': r'^cà rốt',\n",
    "    'onion': r'hành tây',\n",
    "    'tomato': r'^cà chua(?!.*(puree|đặc))',\n",
    "    'cabbage': r'bắp cải trắng',\n",
    "    'banana': r'^chuối(?!.*sấy)',\n",
    "    'orange': r'^cam\\b(?!.*sấy)',\n",
    "    'milk': r'^sữa (tươi|tiệt|dinh|vina)(?!.*(melon|chuối|trái cây|có đường|ít đường|soco|dâu|vani|trân châu|ngữ|choco|lacto))',\n",
    "    'yogurt': r'^sữa chua(?!.*(uống|men|khô|dẻo|ml))',\n",
    "    'condensed_milk': r'sữa đặc(?!.*xanh lá)',\n",
    "    'black_tea': r'^(hồng trà|trà\\b)(?!.*(ml|l\\b|xanh|sữa|khổ|sen|atiso|hoa cúc|ô long|olong|o long|green|ice|nestea|thảo|gừng|lài|matcha|chia|sâm|thế hệ|hà thủ|thái nguyên|15g|blendy|linh chi|happy|tân cương|huế|tết|thanh nhiệt))',\n",
    "    'green_tea': r'^trà\\b(?!.*(ml|l\\b|sữa|khổ|atiso|hoa cúc|ice|nestea|thảo|gừng|matcha|chia|thế hệ|hà thủ|blendy|linh chi|happy|huế|thanh nhiệt|dilmah|twinings|tết|tim sen|chanh|tâm sen|đen|lipton|dâu|bạc hà|hàn quốc|đào|quất))',\n",
    "    'ground_coffee': r'^(cà phê|cafe)(?!.*(hòa tan|hoà tan|sữa|in1|nesca|hạt|425g|bịch|fin|cino|hương))', # matches ground coffees, but excludes instant coffees and coffee with additives\n",
    "    'sugar': r'^đường\\s(tinh|trắng|mía|kính)',\n",
    "    'salt': r'^muối(?!.*(tôm|ớt|tiêu)).*(biển|iot|tinh|sạch)',\n",
    "    'sunflower_oil': r'^dầu.*hướng dương',\n",
    "    'soybean_oil': r'dầu.*nành',\n",
    "    'water': r'nước\\s(uống đóng|khoáng|tinh)(?!.*(ion|chanh|perr))',\n",
    "    'spaghetti': r'^mì(?!.*(kool|trộn|bò|omto|kem)).*(ý|spag|hair|buca)',\n",
    "    'rice_noodles': r'^(bún|phở)(?!.*(lứt|đen|60g|65g|\\sg$)).*(wai|minh hảo|nuffam|bình tây|sa đéc|saf|select|mikiri|hùng lô)',\n",
    "    'tofu': r'^(đậu|tàu)\\shũ(?!.*(chiên|trứng|cá\\b|nấm|hạt|ky))', # matches plain tofus, but excludes fried and flavored varieties\n",
    "    'water_spinach': r'^rau.*muống',\n",
    "    'mango': r'^xoài(?!.*(sấy|ngâm))',\n",
    "    'fish_sauce': r'^nước mắm(?!.*(ớt|me\\b|gừng|chua\\b|chay|tỏi|ngừ|nục|ăn liền))'\n",
    "}\n",
    "product_regex_list = '|'.join(product_regex_map.values()) # create a single regex by joining all individual regexes with the OR operator (|)\n",
    "\n",
    "# filter products matching any of the product types\n",
    "filtered_products = products.loc[products.name.str.contains(product_regex_list, case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc0972-ab6d-41da-98e4-78af7f8119a0",
   "metadata": {},
   "source": [
    "#### 3.3. Assigning Product Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded9e9-f1a1-4297-afee-77608dfc29e8",
   "metadata": {},
   "source": [
    "Each filtered product is tagged with its corresponding product type by matching its name against the predefined regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b610ef6-6b31-4a16-ad2a-c5d96f6d0367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_product_type(row):\n",
    "    name = row['name']\n",
    "    for product_type, regex in product_regex_map.items():\n",
    "        match = re.search(regex, name, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return product_type\n",
    "    return None\n",
    "\n",
    "filtered_products = filtered_products.copy()  # recreate the dataframe\n",
    "filtered_products.loc[:,'product_type'] = filtered_products.apply(assign_product_type, axis=1)\n",
    "\n",
    "# optional intermediate output of the filtered products list\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561d51e-b56a-4239-9073-5baf086cfd57",
   "metadata": {},
   "source": [
    "#### 3.4. Extracting and Normalizing Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f26a5-561e-4f74-91d4-a7b64d9e6cd1",
   "metadata": {},
   "source": [
    "Many product listings differ in quantity, weight, or volume. To enable a fair comparison, we extract the relevant information from the product name or pricing clarification field and calculate normalized price metrics such as price per kilogram, per liter, or per unit.\n",
    "\n",
    "- Weight in grams\n",
    "- Number of units (in particular, eggs)\n",
    "- Volume in milliliters\n",
    "\n",
    "Each value is extracted using pattern matching. Not all products contain all values, so some normalization columns (e.g., *price_kg*, *price_lit*, *price_unit*) may be missing depending on the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737994f-0f7c-44b8-b237-17d7fc3c96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(row):\n",
    "    \"\"\"Extracts total weight in grams from the product name or unit info.\n",
    "    Supports single weights and multi-portion formats (e.g., '5x100g', '5 gói x 100g').\n",
    "    If only 'kg' is mentioned or implied, defaults to 1kg.\n",
    "    \"\"\"\n",
    "    \n",
    "    name, uom = row['name'], row['uom']\n",
    "    \n",
    "    # multi-portion format (weight goes first)\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(g\\b|gr\\b)\\s?(x|gói)\\s?(\\d+)', name, flags=re.IGNORECASE) # matches digits g х digits\n",
    "    if match:\n",
    "        portion = int(match.group(4)) # extract the number of portions\n",
    "        per_portion = float(match.group(1).replace(',', '.')) # extract the weight per portion\n",
    "        return portion * per_portion # return total weight\n",
    "    # multi-portion format (weight goes second)    \n",
    "    match = re.search(r'(\\d+)(\\s|\\shủ\\s?|\\shộp\\s?|\\sgói\\s?|\\stúi\\s?)?x\\s?(\\d+|\\d+[,.]\\d+)\\s?g\\b', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        portion = float(match.group(1).replace(',', '.'))\n",
    "        per_portion = float(match.group(3).replace(',', '.'))\n",
    "        return portion * per_portion\n",
    "    # single weight (grams or kilograms)\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(g\\b|gr\\b|kg)', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        weight = float(match.group(1).replace(',', '.'))\n",
    "        unit = match.group(2)\n",
    "        return weight * 1000 if unit in ['kg','Kg'] else weight # convert kilograms to grams\n",
    "    # if name doesn't contain anything, check uom\n",
    "    if uom == 'kg':\n",
    "        weight = 1000\n",
    "        return weight\n",
    "    # if none of above worked but there's 'kg' in the name\n",
    "    match = re.search(r'kg', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        weight = 1000\n",
    "        return weight\n",
    "    \n",
    "    return None  # if nothing matched\n",
    "\n",
    "# the next two functions follow the same logic as extract_weight, but for units and milliliters\n",
    "def extract_number_of_units(row):\n",
    "    \"\"\"Extracts number of units from the product name or clarification.\n",
    "    Supports formats like '10x', '10 túi', '10 trứng'.\n",
    "    \"\"\"\n",
    "    \n",
    "    name, product_type, uom = row['name'], row['product_type'], row['uom']\n",
    "    \n",
    "    # check name\n",
    "    match = re.search(r'(\\d+)\\s?(túi|gói|trứng|t\\b|x)', name, flags=re.IGNORECASE) # túi - bag, gói - package, trứng/t - egg\n",
    "    if match:\n",
    "        number_of_units = int(match.group(1))\n",
    "        return number_of_units\n",
    "   \n",
    "def extract_volume(row):\n",
    "    \"\"\"Extracts total volume in milliliters from the product name.\n",
    "    Supports single and multi-portion formats (e.g., '5x100ml', 'thùng 6 x 330ml').\n",
    "    \"\"\"\n",
    "    \n",
    "    name, uom = row['name'], row['uom']\n",
    "    \n",
    "    # multi-portion format (volume goes first)\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(ml|l\\b|lít)\\s?(x|thùng)\\s?(\\d+)', name, flags=re.IGNORECASE) # thùng - box\n",
    "    if match:\n",
    "        portion = int(match.group(4))\n",
    "        per_portion = float(match.group(1).replace(',', '.'))\n",
    "        unit = match.group(2)\n",
    "        return portion * per_portion * 1000 if unit in ['l', 'L', 'lít'] else portion * per_portion\n",
    "    # multi-portion format (volume goes second)    \n",
    "    match = re.search(r'(\\d+)(\\s|\\sgói\\s?|\\sbịch\\s?|\\shộp\\s?|\\schai\\s?)?[x×]\\s?(\\d+|\\d+[,.]\\d+)\\s?(ml|l\\b|lít)', name, flags=re.IGNORECASE) # bịch - bag, hộp - box, chai - bottle\n",
    "    if match:\n",
    "        portion = int(match.group(1))\n",
    "        per_portion = float(match.group(3).replace(',', '.'))\n",
    "        unit = match.group(4)\n",
    "        return portion * per_portion * 1000 if unit in ['l', 'L', 'lít'] else portion * per_portion\n",
    "    # single volume (liters or milliliters)\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(ml|l\\b|lít)', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        volume = float(match.group(1).replace(',', '.'))\n",
    "        unit = match.group(2)\n",
    "        return volume * 1000 if unit in ['l', 'L', 'lít'] else volume\n",
    "    \n",
    "    return None  # if nothing matched\n",
    "\n",
    "filtered_products = filtered_products.copy()  # recreate the dataframe\n",
    "\n",
    "# calculate normalized prices\n",
    "filtered_products.loc[:,'weight'] = filtered_products.apply(extract_weight, axis=1)  # a column with weigths in grams\n",
    "filtered_products.loc[:,'price_kg'] = filtered_products.price / filtered_products.weight * 1000   # a column with prices per kg\n",
    "\n",
    "filtered_products.loc[:,'number_of_units'] = filtered_products.apply(extract_number_of_units, axis=1)  # a column with number of units\n",
    "filtered_products.loc[:,'price_unit'] = filtered_products.price / filtered_products.number_of_units   # a column with prices per unit\n",
    "\n",
    "filtered_products.loc[:,'volume'] = filtered_products.apply(extract_volume, axis=1)  # a column with volume in ml\n",
    "filtered_products.loc[:,'price_lit'] = filtered_products.price / filtered_products.volume * 1000   # a column with prices per liter\n",
    "\n",
    "# optional intermediate output\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d1a56-181c-434a-9fee-0bd9340f7e2a",
   "metadata": {},
   "source": [
    "#### 3.5. Saving the Final Filtered Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf9f31-9fba-471f-9a04-df5f93194b0b",
   "metadata": {},
   "source": [
    "Finally, the enriched dataset is saved to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0475b-0eae-43d8-8d4d-62324e3fbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_products.to_csv(f'filtered_products-2025-03-07-coop.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
