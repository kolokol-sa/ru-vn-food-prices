{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde7cd50-703f-4771-bb42-04b899113438",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a9bc1-b1a0-4547-bbf4-2f1dc3e67505",
   "metadata": {},
   "source": [
    "We import the necessary libraries and define key parameters for interacting with Winmart’s API: the base URL for fetching categories, the current date for reference, and request headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357155d0-4f45-48b3-86a7-9f4486be3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "# define base URLs and headers for making requests throughout the scraping process\n",
    "CATEGORIES_URL = \"https://api-crownx.winmart.vn/mt/api/web/v1/category\"\n",
    "CHECK_DATE = date.today() # current date for reference\n",
    "\n",
    "HEADERS = {\n",
    "    'origin': 'https://winmart.vn',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92351187-8d1b-4731-b66e-8f1faf8551bf",
   "metadata": {},
   "source": [
    "### 2. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5f5db-8226-476a-adb4-fd988806c981",
   "metadata": {},
   "source": [
    "To extract product data from Winmart, we follow a multi-step process: fetch product categories, collect item data by category, clean the results, and save everything in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eadb90-bad2-46d7-8267-43720272a2f7",
   "metadata": {},
   "source": [
    "#### 2.1. Fetching Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe11cbd-ac16-424a-9426-027c293cc7b5",
   "metadata": {},
   "source": [
    "We start by retrieving a list of product categories from Winmart’s API. The function extracts relevant fields (*code*, *name*, parent category, a flag indicating whether the category has children, *slug* – used to construct URLs and hierarchy *level*) and returns a cleaned list.\n",
    "\n",
    "Each category is identified by a numeric code and a slug. While the numeric code is not directly needed for the product requests, the slug is essential for constructing the URL to fetch product data from specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe956d8-332a-4772-98b8-deb105f60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories():\n",
    "    \n",
    "    response = requests.get(CATEGORIES_URL, headers=HEADERS) # fetch category data from the API endpoint\n",
    "    raw_categories = json.loads(response.text) # convert JSON response to a list of category dictionaries\n",
    "\n",
    "    cleaned_categories = []\n",
    "    \n",
    "    # loop through top-level categories and extract relevant fields\n",
    "    for category in raw_categories[\"data\"]:\n",
    "        cleaned_categories.append({\n",
    "            \"code\": category[\"parent\"][\"code\"],  # numeric code for further product fetching\n",
    "            \"name\": category[\"parent\"][\"name\"],  # name for reference\n",
    "            \"parent_id\": None,   # id and name of a parent category for reference \n",
    "            \"parent_name\": None, # there's no parent for the top level, but we add the field for consistency\n",
    "            \"has_child\": True if category[\"lstChild\"] else False, # True if the category has child categories, important for the products fetching\n",
    "            \"slug\": category[\"parent\"][\"seoName\"], # category name used in requests, for further products fetching\n",
    "            \"level\": category[\"parent\"][\"level\"] # level of category for reference\n",
    "        })\n",
    "        if category[\"lstChild\"]:    # if category has children, go through them too\n",
    "            for child in category[\"lstChild\"]:\n",
    "                cleaned_categories.append({\n",
    "                    \"code\": child[\"parent\"][\"code\"],\n",
    "                    \"name\": child[\"parent\"][\"name\"],\n",
    "                    \"parent_id\": category[\"parent\"][\"code\"],\n",
    "                    \"parent_name\": category[\"parent\"][\"name\"],\n",
    "                    \"has_child\": False,\n",
    "                    \"slug\": child[\"parent\"][\"seoName\"],\n",
    "                    \"level\": child[\"parent\"][\"level\"]\n",
    "                })\n",
    "\n",
    "    return cleaned_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3dc24-380b-4fb5-8d1f-1d54873ac51d",
   "metadata": {},
   "source": [
    "All available categories are saved to a CSV file for transparency and reproducibility. For product scraping, we use only the lowest-level (childless) categories — these are smaller and more specific, reducing the risk of timeouts or rate-limiting during batch requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d945e47-de47-4533-8aa7-648c53610ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = fetch_categories()\n",
    "categories_df = pd.DataFrame(categories)\n",
    "categories_df.to_csv(f'categories-{CHECK_DATE}-winmart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd94b2-09eb-4bfa-af19-70605a02e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional intermediate output of the categories list\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(categories_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619d3a5-595c-481d-8298-06439601407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_end = [cat for cat in categories if not cat['has_child']] # select only childless categories\n",
    "# categories_end = categories_end[:] # this line can be used to resume scraping from a specific point in case of interruption\n",
    "total_categories_count = len(categories_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abcb46-37b9-4052-8136-063ab2832642",
   "metadata": {},
   "source": [
    "#### 2.2. Fetching Raw Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b54c4-039f-42e6-9ace-7686ec77b8d4",
   "metadata": {},
   "source": [
    "We retrieve product listings using the site’s API, which returns data in paginated batches. Each request requires the category’s slug, page number, and other parameters like page size (usually 8 products per page), store code, and store group code.\n",
    "\n",
    "The API fetches the first 8 products when a user visits the category page, and as the user scrolls, additional requests are made to fetch subsequent pages. The first page request includes the total number of pages, allowing us to determine how many requests to make for a given category.\n",
    "\n",
    "We loop through all pages in a given category, adding a random delay between requests to reduce the risk of being rate-limited or blocked. The result is raw product data in JSON format, consistent with the site’s internal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553fe2a-603c-42c7-ada8-51431ceb86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_products(category):\n",
    "    \n",
    "    products = []\n",
    "\n",
    "    # define URL and parameters for the request\n",
    "    ITEMS_URL = \"https://api-crownx.winmart.vn/it/api/web/v3/item/category\"\n",
    "    PARAMS = {\n",
    "            'orderByDesc': 'true',\n",
    "            'pageNumber': '1',\n",
    "            'pageSize': '8', # set to 8, consistent with the site's observed pagination behavior\n",
    "            'slug': category[\"slug\"],\n",
    "            'storeCode': '1561',\n",
    "            'storeGroupCode': '1999',\n",
    "        }\n",
    "    \n",
    "    response = requests.get(ITEMS_URL, headers=HEADERS, params=PARAMS) # fetch the first page of product data\n",
    "    response_data = json.loads(response.text)\n",
    "    total_pages = response_data[\"paging\"][\"totalPages\"] # retrieve the total number of product pages for looping\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        \n",
    "        time.sleep(random.uniform(1, 5)) # random delay to avoid rate-limiting or blocking\n",
    "        \n",
    "        PARAMS = {\n",
    "            'orderByDesc': 'true',\n",
    "            'pageNumber': str(page),\n",
    "            'pageSize': '8',\n",
    "            'slug': category['slug'],\n",
    "            'storeCode': '1561',\n",
    "            'storeGroupCode': '1999',\n",
    "        }\n",
    "    \n",
    "        response = requests.get(ITEMS_URL, headers=HEADERS, params=PARAMS) # fetch product data for the current page\n",
    "        response_data = json.loads(response.text)\n",
    "        products += response_data[\"data\"][\"items\"] # extract only products and add them to the list\n",
    "        \n",
    "        print(f'{len(products)}..', end='') # progress indication\n",
    "\n",
    "    return products   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42985df1-e564-46a3-a383-84aec39147ba",
   "metadata": {},
   "source": [
    "#### 2.3. Cleaning Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbf0f4-3784-4018-8953-a49e8aaef5c8",
   "metadata": {},
   "source": [
    "We define a function to extract only the most relevant fields from the raw data:\n",
    "\n",
    "- Category code (for traceability)\n",
    "- Product name\n",
    "- Price\n",
    "- Unit of measurement\n",
    "- Supermarket name\n",
    "\n",
    "The result is a cleaned list of product entries, ready for storage or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68de3af-01af-4979-8113-93757e5df6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_data(category, raw_products):\n",
    "\n",
    "    cleaned_products = []\n",
    "\n",
    "    # loop through raw product entries and retain only the required fields\n",
    "    for product in raw_products:\n",
    "        cleaned_products.append({\n",
    "            \"category_code\": category[\"code\"],\n",
    "            \"name\": product[\"name\"],\n",
    "            \"price\": product[\"price\"],\n",
    "            \"uom\": product[\"uom\"],\n",
    "            \"supermarket\": 'Winmart' # hardcoded source name for clarity during analysis\n",
    "        })\n",
    "\n",
    "    return cleaned_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9d8a9-14c0-4d00-a79a-d641832322d0",
   "metadata": {},
   "source": [
    "#### 2.4. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276533e-40ad-4626-8d44-bad9a645c2e5",
   "metadata": {},
   "source": [
    "The following code:\n",
    "\n",
    "1. Iterates through the selected lowest-level categories (i.e., categories without children),\n",
    "2. Fetches and cleans product data for each,\n",
    "3. Appends the cleaned data to a single CSV file to build a complete dataset.\n",
    "\n",
    "A progress tracker prints a summary after each category to monitor scraping progress.\n",
    "A random delay is added between requests to reduce the risk of being blocked.\n",
    "The output filename includes a timestamp (via `CHECK_DATE`) for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625c17e-d516-467a-b13f-42ed9870bfc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fetched_categories_count = 0    # counter for fetching progress tracker\n",
    "\n",
    "# create the file with headers first\n",
    "products_df = pd.DataFrame(columns=[\"category_code\", \"name\", \"price\", \"uom\", \"supermarket\"])\n",
    "products_df.to_csv(f'scraped_products-{CHECK_DATE}-winmart.csv', index=False, mode='w')\n",
    "\n",
    "for category in categories_end:\n",
    "    \n",
    "    raw_products = fetch_products(category) # fetch products\n",
    "    new_products = clean_product_data(category, raw_products) # select only relevant data and add new products to the list\n",
    "\n",
    "    products_df = pd.DataFrame(new_products)\n",
    "    products_df.to_csv(f'scraped_products-{CHECK_DATE}-winmart.csv', index=False, mode='a', header=False)\n",
    "\n",
    "    fetched_categories_count += 1\n",
    "    print(f'Category ID: {category[\"code\"]} finished, {fetched_categories_count} out of {total_categories_count} categories fetched')\n",
    "    \n",
    "    time.sleep(random.uniform(1, 5))\n",
    "\n",
    "print(f'Fetching complete. Results saved to scraped_products-{CHECK_DATE}-winmart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156663e5-e5e8-4627-ac1e-86f9a37fd0a9",
   "metadata": {},
   "source": [
    "### 3. Filtering and Normalizing Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da31242-f91f-4db5-bc03-be2097be620d",
   "metadata": {},
   "source": [
    "After collecting and cleaning the raw product data, we proceed with filtering the dataset to include only the products relevant for comparison. This step involves several stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c9388-60b1-4115-a842-ab4366bcea46",
   "metadata": {},
   "source": [
    "#### 3.1. Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7d6e8-fc83-4b99-86db-236b1f90fc44",
   "metadata": {},
   "source": [
    "We begin by loading the previously saved product and category datasets.\n",
    "- The `category_code` column is dropped as it is no longer needed for the next steps.\n",
    "- Duplicate product entries are removed. These may appear if a product belonged to multiple categories.\n",
    "- Extra spaces in product names are stripped to ensure consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9763d85-3a2d-48d0-9459-da05ce72cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "categories = pd.read_csv('categories-2025-04-17-winmart.csv')\n",
    "products_original = pd.read_csv('scraped_products-2025-04-17-winmart.csv')\n",
    "\n",
    "products = products_original.drop(['category_code'], axis=1)    # drop category_code column\n",
    "products = products.drop_duplicates() # remove duplicates\n",
    "products['name'] = products['name'].str.strip() # strip extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fd19d-1a65-4885-9ea1-deb2dace1953",
   "metadata": {},
   "source": [
    "#### 3.2. Filtering Products by Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b30a2-fbb2-469d-ac53-0cabc4c12209",
   "metadata": {},
   "source": [
    "To identify relevant products for comparison, we define a dictionary mapping product types to regular expressions. Each expression captures the base form of the product while deliberately excluding variations (e.g., flavored, processed, or pickled) that fall outside the scope of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a428c-a3ed-4f8e-aa7d-483d24ccdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_regex_map = {\n",
    "    'rice': r'^gạo(?!.*lứt)',\n",
    "    'bread': r'^bánh (mì|mỳ|sandwich)(?!.*(bơ|hoa|chà|thịt))',\n",
    "    'chicken_fillet': r'(fillet|phi lê|\\bức)(?!.*đùi).*gà',\n",
    "    'pork_leg': r'đùi.*heo',\n",
    "    'egg': r'^trứng gà',\n",
    "    'cucumber': r'^dưa.*(chuột|leo)(?!.*ngâm)',\n",
    "    'carrot': r'^cà rốt',\n",
    "    'onion': r'^hành tây',\n",
    "    'tomato': r'^cà chua',\n",
    "    'cabbage': r'bắp cải trắng', # matches \"bắp cải trắng\", but excludes others like \"bắp cải tím\"\n",
    "    'eggplant': r'cà tím',\n",
    "    'banana': r'^chuối(?!.*sấy)',\n",
    "    'orange': r'^cam',\n",
    "    'milk': r'^(thùng.*sữa|sữa|lốc.*sữa)(?!.*(vị|hương|có đường|ít đường|socola|sô cô la|dâu|chua|lên men|bắp|lact|yến)).*(trùng|tươi|tự nhiên)', # matches plain fresh milk, but excludes flavored, sweetened, fermented, or powdered varieties\n",
    "    'yogurt': r'^(sữa chua|lốc.*hộp.*sữa chua)(?!.*(uống|lên men|fristi|chai))',\n",
    "    'condensed_milk': r'sữa đặc',\n",
    "    'black_tea': r'^trà\\b(?!.*(ml|l\\b|lít|sữa|gừng|nestea|atiso|ice|xanh|ô long|nhài|tôm|thái|hòa tan|zoga))', # matches teas, but excludes all green varieties, powdered tea and bottled drinks\n",
    "    'green_tea': r'^trà\\b(?!.*(ml|l\\b|lít|sữa|gừng|nestea|atiso|ice|đen|hòa tan|zoga|hoa cúc|ceylon|đào))',\n",
    "    'ground_coffee': r'^(cà phê|café)(?!.*hòa tan).*(bột|xay|sáng|chế phin|nâu|khát)',\n",
    "    'sugar': r'^đường.*(pure|trắng|mía)',\n",
    "    'salt': r'^muối.*biển', # matches \"muối biển\", but excludes mixed salts like \"muối tôm\"\n",
    "    'sunflower_oil': r'dầu.*hướng dương',\n",
    "    'soybean_oil': r'dầu.*nành',\n",
    "    'water': r'^nước (uống|khoáng|tinh)(?!.*(vị|sữa|tăng|ion))',\n",
    "    'spaghetti': r'^mì(?!.*(kool|xốt)).*(ý|spag)',\n",
    "    'rice_noodles': r'^bún.*safoco',\n",
    "    'tofu': r'^(đậu|tàu) hũ(?!.*(chiên|trứng))',\n",
    "    'water_spinach': r'^rau.*muống',\n",
    "    'mango': r'^xoài(?!.*sấy)',\n",
    "    'fish_sauce': r'^nước mắm(?!.*ớt)'\n",
    "}\n",
    "product_regex_list = '|'.join(product_regex_map.values()) # create a single regex by joining all individual regexes with the OR operator (|)\n",
    "\n",
    "# filter products matching any of the product types\n",
    "filtered_products = products.loc[products.name.str.contains(product_regex_list, case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26559e12-18a5-4204-a146-1f6afa53f800",
   "metadata": {},
   "source": [
    "#### 3.3. Assigning Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f6044-3f4d-4898-81a2-5df42ff2e3a3",
   "metadata": {},
   "source": [
    "Each filtered product is tagged with its corresponding product type based on regex matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c442a-3595-4e76-9515-95ee29d09171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_product_type(row):\n",
    "    name = row['name']\n",
    "    for product_type, regex in product_regex_map.items():\n",
    "        match = re.search(regex, name, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return product_type\n",
    "    return None\n",
    "\n",
    "filtered_products = filtered_products.copy()  # recreate the dataframe\n",
    "filtered_products.loc[:,'product_type'] = filtered_products.apply(assign_product_type, axis=1)\n",
    "\n",
    "# optional intermediate output of the filtered products list\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e4f9a-a700-480f-a4fe-b0deb07d2434",
   "metadata": {},
   "source": [
    "#### 3.4. Extracting and Normalizing Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf512d2-aa14-4e6a-b06b-c7237dee2918",
   "metadata": {},
   "source": [
    "Many product listings differ in quantity, weight, or volume. To enable a fair comparison, we extract the relevant information from the product name or pricing clarification field and calculate normalized price metrics such as price per kilogram, per liter, or per unit.\n",
    "\n",
    "- Weight in grams\n",
    "- Number of units (in particular, eggs)\n",
    "- Volume in milliliters\n",
    "\n",
    "Each value is extracted using pattern matching. Not all products contain all values, so some normalization columns (e.g., *price_kg*, *price_lit*, *price_unit*) may be missing depending on the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c617f1a-5d5a-41df-b680-e2bfd427bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(row):\n",
    "    \"\"\"Extracts total weight in grams from the product name or uom.\"\"\"\n",
    "    \n",
    "    name, uom = row['name'], row['uom']\n",
    "    \n",
    "    # some products are sold in groups denoted in the uom (e.g., 'G3' means 3 units)\n",
    "    match = re.search(r'G(\\d)', uom)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "    elif uom == 'T': # if uom is 'T' (thùng - box) that means it's a box of N packages, the exact number is denoted in the product_name\n",
    "        match = re.search(r'(\\d+)\\s?(gói|hộp|túi)', name, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "    else:\n",
    "        number = 1 # set to 1 if no grouping was found\n",
    "    # check if name specifies weight\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(g\\b|kg)', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        weight = float(match.group(1).replace(',', '.')) # extract the weight\n",
    "        unit = match.group(2)\n",
    "        return number * weight * 1000 if unit in ['kg','Kg'] else number * weight # convert kilograms to grams if needed\n",
    "    # check uom\n",
    "    if uom == 'KG':\n",
    "        weight = 1000\n",
    "        return weight\n",
    "    \n",
    "    return None  # if nothing matched\n",
    "\n",
    "# the next two functions follow the similar logic as extract_weight, but for units and milliliters\n",
    "def extract_number_of_units(row):\n",
    "    \"\"\"Extracts number of units from the product name or pricing clarification\n",
    "    (quả, trái - piece).\n",
    "    \"\"\"\n",
    "    \n",
    "    name, product_type, uom = row['name'], row['product_type'], row['uom']\n",
    "    \n",
    "    # check name\n",
    "    match = re.search(r'(\\d+)\\s?(quả|trái)', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        number_of_units = int(match.group(1))\n",
    "        return number_of_units\n",
    "    # for eggs, take any number from the name (because this is how Winmart specifies it)\n",
    "    match = re.search(r'(\\d+)', name)\n",
    "    if match and product_type == 'egg':\n",
    "        number_of_units = int(match.group(1))\n",
    "        return number_of_units\n",
    "    \n",
    "    return None  # if nothing matched\n",
    "\n",
    "def extract_volume(row):\n",
    "    \"\"\"Extracts total volume in milliliters from the product name or pricing clarification.\"\"\"\n",
    "    \n",
    "    name, uom = row['name'], row['uom']\n",
    "    \n",
    "    # check groups\n",
    "    match = re.search(r'G(\\d)', uom)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "    elif uom == 'T':\n",
    "        match = re.search(r'(\\d+)\\s?(gói|hộp|túi|chai)', name, flags=re.IGNORECASE) # (hộp - box, túi - bag, chai - bottle)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "        else:\n",
    "            number = 0\n",
    "    else:\n",
    "        number = 1\n",
    "    # chech if name specifies volume\n",
    "    match = re.search(r'(\\d+|\\d+[,.]\\d+)\\s?(ml|l\\b|lít)', name, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        volume = float(match.group(1).replace(',', '.'))\n",
    "        unit = match.group(2)\n",
    "        return number * volume * 1000 if unit in ['l', 'L', 'lít'] else number * volume\n",
    "\n",
    "    return None  # if nothing matched\n",
    "\n",
    "filtered_products = filtered_products.copy()  # recreate the dataframe\n",
    "\n",
    "# calculate normalized prices\n",
    "filtered_products.loc[:,'weight'] = filtered_products.apply(extract_weight, axis=1)  # a column with weigths in grams\n",
    "filtered_products.loc[:,'price_kg'] = filtered_products.price / filtered_products.weight * 1000   # a column with prices per kg\n",
    "\n",
    "filtered_products.loc[:,'number_of_units'] = filtered_products.apply(extract_number_of_units, axis=1)  # a column with number of units\n",
    "filtered_products.loc[:,'price_unit'] = filtered_products.price / filtered_products.number_of_units   # a column with prices per unit\n",
    "\n",
    "filtered_products.loc[:,'volume'] = filtered_products.apply(extract_volume, axis=1)  # a column with volume in ml\n",
    "filtered_products.loc[:,'price_lit'] = filtered_products.price / filtered_products.volume * 1000   # a column with prices per liter\n",
    "\n",
    "# optional intermediate output\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e0184-e619-4342-bc55-987fc8b9d36e",
   "metadata": {},
   "source": [
    "#### 3.5. Saving the Final Filtered Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dba959-3014-40d3-8c82-d986901d0913",
   "metadata": {},
   "source": [
    "Finally, the enriched dataset is saved to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d5d6d-8ed4-4320-88d8-a340ffa2280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_products.to_csv(f'filtered_products-2025-04-17-winmart.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
