{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e28f2fd-4674-4c58-b60c-6dc6ac08ed94",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a602a-f8ab-43bd-a850-f7f1ff2733bf",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries and defining key parameters for scraping product data from the Pyaterochka’s online store. This includes base URLs for making requests to the store's API, headers to simulate a browser request, and the current date for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357155d0-4f45-48b3-86a7-9f4486be3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "# define base URLs and headers for making requests throughout the scraping process\n",
    "BASE_URL = \"https://5d.5ka.ru/api/catalog/v2/stores/Y233/categories/\"\n",
    "CATEGORIES_URL = \"https://5d.5ka.ru/api/catalog/v2/stores/Y233/categories?mode=delivery&include_subcategories=1\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"\n",
    "CHECK_DATE = date.today() # current date for reference\n",
    "\n",
    "HEADERS = {\n",
    "    \"user-agent\": USER_AGENT,\n",
    "    \"origin\": \"https://5ka.ru\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000dbaa-0343-4f93-9d7f-45f5dcb9b55a",
   "metadata": {},
   "source": [
    "### 2. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015df7e-edc1-4c11-ba48-01b7703289bd",
   "metadata": {},
   "source": [
    "To extract product data from the Pyaterochka website, we follow a multi-step process: retrieving product categories, fetching product data by category, cleaning the raw data, and storing the results in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e88bbd-4ca5-45e3-bd5a-8c40685f63da",
   "metadata": {},
   "source": [
    "#### 2.1. Fetching Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228d1f5-454e-4ce5-be0a-b4962befa975",
   "metadata": {},
   "source": [
    "Since products are grouped by categories, we first need to request a list of all available categories and subcategories. The list of categories is fetched from a dedicated API endpoint used by the site’s catalogue view. Each subcategory includes a unique alphanumeric ID, which is later used to query the corresponding product listings.\n",
    "\n",
    "We define a function to query the categories endpoint, extract only the necessary fields (*id*, *name*, and *parent_id*), and return a cleaned list of subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe956d8-332a-4772-98b8-deb105f60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories():\n",
    "    \n",
    "    response = requests.get(CATEGORIES_URL, headers=HEADERS) # fetch category data from the API endpoint\n",
    "    raw_categories = json.loads(response.text) # convert JSON response to a list of category dictionaries\n",
    "    \n",
    "    cleaned_categories = []\n",
    "    \n",
    "    # go through the raw data and select only the necessary fields\n",
    "    for category in raw_categories:    # iterate through each top-level category\n",
    "        for subcategory in category[\"categories\"]:    # go through all its subcategories and save them, specify parent_category\n",
    "            cleaned_categories.append({\n",
    "                \"id\": subcategory[\"id\"], # id for further products fetching\n",
    "                \"name\": subcategory[\"name\"], # name for reference\n",
    "                \"parent_id\": category[\"id\"] # id of a parent category for reference\n",
    "            })\n",
    "\n",
    "    return cleaned_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8203c8-8ca0-4d68-87ca-32b98668c827",
   "metadata": {},
   "source": [
    "We fetch all available categories and save them to a CSV file for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d945e47-de47-4533-8aa7-648c53610ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = fetch_categories()\n",
    "# categories = categories[83:] # this line can be used to resume scraping from a specific point in case of interruption\n",
    "total_categories_count = len(categories) # for fetching progress indication\n",
    "\n",
    "categories_df = pd.DataFrame(categories)\n",
    "categories_df.to_csv(f'categories-{CHECK_DATE}-pyaterochka.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befcb3b-d13b-4a7a-83bc-43b197b799e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional intermediate output of the categories list\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326c5af-6c00-4075-9f35-7a0badf36a6e",
   "metadata": {},
   "source": [
    "#### 2.2. Fetching Raw Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571c725-4169-46d1-bd8e-d84626af6e39",
   "metadata": {},
   "source": [
    "The Pyaterochka website dynamically loads product data through GET requests tied to specific category codes. Each category page uses a unique alphanumeric identifier, included in the URL when fetching product data. The site first loads 12 items and then increases the number incrementally (e.g., 24, 36, 48…) as the user scrolls.\n",
    "To replicate this behavior in our scraping process, we increment the limit parameter with each request and keep replacing the product list with the most complete version returned so far, until no more new products are added. A random delay is included between requests to avoid being rate-limited or blocked.\n",
    "\n",
    "This step returns raw product data in JSON format, preserving the site’s original structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff0bf8-4b3d-4911-9e12-59047912b228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_products(category_id):\n",
    "    \n",
    "    current_limit = 0\n",
    "    total_products = 0\n",
    "    products = []\n",
    "    \n",
    "    while total_products >= current_limit:\n",
    "        \n",
    "        time.sleep(random.uniform(1, 5)) # random time delay to avoid being blocked\n",
    "        \n",
    "        current_limit += 12 # increase the limit to fetch more products\n",
    "        url = f'{BASE_URL}{category_id}/products?mode=delivery&include_restrict=false&limit={current_limit}'\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS) # fetch product data for the current category and limit\n",
    "        response_data = json.loads(response.text) # convert into a dictionary\n",
    "        \n",
    "        products = response_data[\"products\"] # extract only products\n",
    "        total_products = len(products) # check the total number of products\n",
    "        \n",
    "        print(f'{len(products)}..', end='') # progress indicator, showing the number of products fetched in the current request\n",
    "\n",
    "    return products    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da239f-5281-47b2-ba54-afa4531baf12",
   "metadata": {},
   "source": [
    "#### 2.3. Cleaning Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db589ac-c31c-4ccf-b79e-b806008b596b",
   "metadata": {},
   "source": [
    "Next, we define a function to extract only the relevant product fields from the raw data:\n",
    "\n",
    "- Category ID (for traceability)\n",
    "- Product name\n",
    "- Unit of measurement\n",
    "- Regular and discounted prices\n",
    "- Pricing clarification (e.g. net weight or price per unit info)\n",
    "\n",
    "The result is a cleaned list of product entries ready for storage or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e9901-c591-41da-938b-3cf9f5a76030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_data(category, raw_products):\n",
    "\n",
    "    cleaned_products = []\n",
    "\n",
    "    # go through the raw data and select only the necessary fields\n",
    "    for product in raw_products:\n",
    "        cleaned_products.append({\n",
    "            \"category_id\": category,\n",
    "            \"name\": product[\"name\"],\n",
    "            \"unit_of_measurement\": product[\"uom\"],\n",
    "            \"price_reg\": product[\"prices\"][\"regular\"],\n",
    "            \"price_disc\": product[\"prices\"][\"discount\"],\n",
    "            \"pricing_clarification\": product[\"property_clarification\"] # clarifies the unit for the price or the net weight\n",
    "        })\n",
    "\n",
    "    return cleaned_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723adc64-dea5-498d-b3c2-ccdd21677fd5",
   "metadata": {},
   "source": [
    "#### 2.4. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cfd69-5e44-47e2-9b8a-fecc03d60e68",
   "metadata": {},
   "source": [
    "In the main loop, we:\n",
    "\n",
    "1. Iterate through the selected categories,\n",
    "2. Fetch and clean product data for each,\n",
    "3. The cleaned data for each category is appended to the same CSV file to build a complete dataset.\n",
    "\n",
    "A progress tracker prints feedback for each category to help monitor the scraping process. We include a short random delay between requests here too to avoid potential rate-limiting. A timestamp is added to the filename using the current date (`CHECK_DATE`) to track when the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fd336-2477-43a0-9b3d-7e0cde4bbfcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fetched_categories_count = 0    # counter for fetching progress tracker\n",
    "\n",
    "# create the file with headers first\n",
    "products_df = pd.DataFrame(columns=[\"category_id\", \"name\", \"unit_of_measurement\", \"price_reg\", \"price_disc\", \"pricing_clarification\"])\n",
    "products_df.to_csv(f'scraped_products-{CHECK_DATE}-pyaterochka.csv', index=False, mode='w')\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    raw_products = fetch_products(category[\"id\"]) # fetch products\n",
    "    new_products = clean_product_data(category[\"id\"], raw_products) # select only relevant data and add new products to the list\n",
    "\n",
    "    products_df = pd.DataFrame(new_products)\n",
    "    products_df.to_csv(f'scraped_products-{CHECK_DATE}-pyaterochka.csv', index=False, mode='a', header=False)\n",
    "\n",
    "    fetched_categories_count += 1\n",
    "    print(f'Category ID: {category[\"id\"]} finished, {fetched_categories_count} out of {total_categories_count} categories fetched')\n",
    "    \n",
    "    time.sleep(random.uniform(1, 5))\n",
    "\n",
    "print(f'Fetching complete. Results saved to scraped_products-{CHECK_DATE}-pyaterochka.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eafb66-2698-406a-86f9-376deed9a027",
   "metadata": {},
   "source": [
    "### 3. Filtering and Normalizing Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a3748-d97c-48da-af36-afd938909fe8",
   "metadata": {},
   "source": [
    "After collecting and cleaning the raw product data, we proceed with filtering the dataset to include only the products relevant for comparison. This step involves several stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c7228-31ef-4e1f-8aa7-0dec7bab8734",
   "metadata": {},
   "source": [
    "#### 3.1. Initial Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976ec20-40fc-4da2-a1c5-47b4fff86970",
   "metadata": {},
   "source": [
    "We start by loading the previously saved product and category datasets and dropping columns that are no longer needed. The column names are simplified for ease of further processing. Also we remove exact duplicates, which may appear if the same item was in more than one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbb8a0-7e8e-4fc1-83c0-e53cd4ba0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "categories = pd.read_csv('categories-2025-03-03-pyaterochka.csv')\n",
    "products_original = pd.read_csv('scraped_products-2025-03-03-complete.csv')\n",
    "\n",
    "# drop unnecessary columns and rename for simplicity\n",
    "products = products_original.drop(['category_id', 'price_disc', 'unit_of_measurement'], axis=1)    # drop category_id, uom and price_disc columns\n",
    "products = products.rename(columns={'price_reg': 'price', 'pricing_clarification': 'pricing_unit'})    # rename price column for simplicity\n",
    "products = products.drop_duplicates() # remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c30f-a8e0-459c-ba82-f1506b65f47a",
   "metadata": {},
   "source": [
    "#### 3.2. Filtering Products by Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babdb703-fbe7-4900-a49c-8642473ab0e7",
   "metadata": {},
   "source": [
    "To identify relevant products for comparison, we define a dictionary mapping product types to regular expressions. Each expression captures the base form of the product while deliberately excluding variations (e.g., flavored, processed, or pickled) that fall outside the scope of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2c9c2-2bff-45e8-b7a3-dac81a527e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_regex_map = {\n",
    "    'rice': r'(^|^\")рис\\b',\n",
    "    'bread': r'(^хлеб\\b|^багет\\b|^батон\\b)(?!.*(чесн|заморож))', # matches \"хлеб\", \"багет\", or \"батон\"\n",
    "                                                                 # but excludes \"багет с чесноком\" or \"багет замороженный\"\n",
    "    'chicken_fillet': r'^филе\\b.*(кур|цыпл)(?!.*запеч)',\n",
    "    'pork_leg': r'^окорок\\b.*свин',\n",
    "    'egg': r'^яйцо.*курин',\n",
    "    'cucumber': r'^огур(цы|ец)(?!.*(солен|маринован))', # matches \"огурец\" or \"огурцы\" but excludes \"огурцы соленые\" or \"огурцы маринованные\"\n",
    "    'carrot': r'^морковь(?!.*корей)',\n",
    "    'onion': r'^лук.*реп(?!.*зелен)',\n",
    "    'tomato': r'^томаты(?!.*(сок|очищ|маринован|вялен|солен))',\n",
    "    'cabbage': r'^капуста\\b.*белокоч',\n",
    "    'eggplant': r'^баклажаны?($|.*теплич)',\n",
    "    'banana': r'^банан',\n",
    "    'orange': r'^апельсин',\n",
    "    'milk': r'^молоко(?!.*(сгущ|сух))',\n",
    "    'yogurt': r'^йогурт\\b(?!.*питье)', # matches \"йогурт\" but excludes \"йогурт питьевой\"\n",
    "    'condensed_milk': r'(^молоко.*сгущ|^сгущ)(?!.*(варен|какао|шокол))',\n",
    "    'green_tea': r'^чай.* зел(?!.*(порош|л$))',\n",
    "    'black_tea': r'^чай.* черн(?!.*л$)',\n",
    "    'ground_coffee': r'^кофе(?!.*(капсул|раствор)).*молот',\n",
    "    'sugar': r'^сахар\\b(?!.*ванил)',\n",
    "    'salt': r'^соль(?!.*(розов|посуд|чесн|ванн|спец))',\n",
    "    'sunflower_oil': r'^масло\\b.*подсолн(?!.*добавл)',\n",
    "    'water': r'^вода(?!.*(малин|лимон)).*негаз',\n",
    "    'buckwheat': r'(^крупа\\b.*гречн|^гречка\\b)(?!.*(\\bпшен|\\bкиноа))',\n",
    "    'spaghetti': r'(^макароны.*спагетти|^спагетти\\b)(?!.*(заморож|кукуруз))',\n",
    "    'rice_noodles': r'(^лапша|^вермишель).*(фунчоз)',\n",
    "    'tofu': r'^тофу\\b',\n",
    "    'mango': r'^манго\\b(?!.*(суш|заморож))'\n",
    "}\n",
    "product_regex_list = '|'.join(product_regex_map.values()) # create a single regex by joining all individual regexes with the OR operator (|)\n",
    "\n",
    "# filter products matching any of the product types\n",
    "filtered_products = products.loc[products.name.str.contains(product_regex_list, case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5a5e4-07e4-403e-b1d6-0ff55f96aac3",
   "metadata": {},
   "source": [
    "#### 3.3. Assigning Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded9e9-f1a1-4297-afee-77608dfc29e8",
   "metadata": {},
   "source": [
    "Each filtered product is tagged with its corresponding product type based on regex matching and is also assigned the supermarket name (“Pyaterochka”) for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b610ef6-6b31-4a16-ad2a-c5d96f6d0367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_product_type(row):\n",
    "    name = row['name']\n",
    "    for product_type, regex in product_regex_map.items():\n",
    "        match = re.search(regex, name, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            return product_type\n",
    "    return None\n",
    "\n",
    "filtered_products = filtered_products.copy()  # create a copy to avoid a warning when adding new columns with .loc\n",
    "filtered_products.loc[:,'product_type'] = filtered_products.apply(assign_product_type, axis=1)\n",
    "filtered_products.loc[:,'supermarket'] = 'Pyaterochka'\n",
    "\n",
    "# optional intermediate output of the filtered products list\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad540b-f6fd-4289-87e0-523ec4065008",
   "metadata": {},
   "source": [
    "#### 3.4. Extracting and Normalizing Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f26a5-561e-4f74-91d4-a7b64d9e6cd1",
   "metadata": {},
   "source": [
    "Many product listings differ in quantity, weight, or volume. To enable a fair comparison, we extract the relevant information from the product name or pricing clarification field and calculate normalized price metrics such as price per kilogram, per liter, or per unit.\n",
    "\n",
    "- Weight in grams\n",
    "- Number of units (in particular, eggs)\n",
    "- Volume in milliliters\n",
    "\n",
    "Each value is extracted using pattern matching. Not all products contain all values, so some normalization columns (e.g., *price_kg*, *price_lit*, *price_unit*) may be missing depending on the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737994f-0f7c-44b8-b237-17d7fc3c96e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_weight(row):\n",
    "    \"\"\"Extracts total weight in grams from the product name or pricing clarification.\n",
    "    Supports both single weights and multi-portion formats (e.g., '5x100г').\n",
    "    \"\"\"\n",
    "    \n",
    "    name, pricing_unit = row['name'], row['pricing_unit']\n",
    "\n",
    "    # multi-portion format (e.g., 5x100г)\n",
    "    match = re.search(r'(\\d+)(x|х)(\\d+|\\d+[.]\\d+)\\s?г', name) # matches digits х digits g\n",
    "    if match:\n",
    "        portion, per_portion = map(float, match.group(1,3)) # extract the number of portions and the weight per portion\n",
    "        return portion * per_portion # return total weight\n",
    "    # single weight (grams or kilograms)\n",
    "    match = re.search(r'(\\d+|\\d+[.]\\d+)\\s?(г|кг)', name)\n",
    "    if match:\n",
    "        weight = float(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        return weight * 1000 if unit == 'кг' else weight # convert kilograms to grams\n",
    "    # if name doesn't contain anything, check pricing_unit\n",
    "    match = re.search(r'(\\d+|\\d+[.]\\d+)\\s?(г|кг)', pricing_unit)\n",
    "    if match:\n",
    "        weight = float(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        return weight * 1000 if unit == 'кг' else weight\n",
    "\n",
    "    return None  # if nothing matched\n",
    "\n",
    "# the next two functions follow the same logic as extract_weight, but for units and milliliters\n",
    "def extract_number_of_units(row):\n",
    "    \"\"\"Extracts number of units ('шт') from the product name or pricing clarification.\"\"\"\n",
    "    \n",
    "    name, pricing_unit = row['name'], row['pricing_unit']\n",
    "\n",
    "    # check name\n",
    "    match = re.search(r'(\\d+)\\s?шт', name)\n",
    "    if match:\n",
    "        number_of_units = int(match.group(1))\n",
    "        return number_of_units\n",
    "    # check pricing_unit\n",
    "    match = re.search(r'(\\d+)\\s?шт', pricing_unit)\n",
    "    if match:\n",
    "        number_of_units = int(match.group(1))\n",
    "        return number_of_units\n",
    "\n",
    "    return None  # if nothing matched\n",
    "\n",
    "def extract_volume(row):\n",
    "    \"\"\"Extracts total volume in milliliters from the product name or pricing clarification.\n",
    "    Supports both single and multi-portion formats (e.g., '5x100мл').\n",
    "    \"\"\"\n",
    "    \n",
    "    name, pricing_unit = row['name'], row['pricing_unit']\n",
    "\n",
    "    # multi-portion format (e.g., 5x100мл)\n",
    "    match = re.search(r'(\\d+)(x|х)(\\d+|\\d+[.]\\d+)\\s?мл', name)\n",
    "    if match:\n",
    "        portion, per_portion = map(float, match.group(1,3))\n",
    "        return portion * per_portion\n",
    "    # single volume (liters or milliliters)\n",
    "    match = re.search(r'(\\d+|\\d+[.]\\d+)\\s?(мл|л\\b)', name)\n",
    "    if match:\n",
    "        volume = float(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        return volume * 1000 if unit == 'л' else volume\n",
    "    # check pricing_unit\n",
    "    match = re.search(r'(\\d+|\\d+[.]\\d+)\\s?(мл|л\\b)', pricing_unit)\n",
    "    if match:\n",
    "        volume = float(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        return volume * 1000 if unit == 'л' else volume\n",
    "\n",
    "    return None  # if nothing matched\n",
    "\n",
    "filtered_products = filtered_products.copy()  # recreate the dataframe\n",
    "\n",
    "# calculate normalized prices\n",
    "filtered_products.loc[:,'weight'] = filtered_products.apply(extract_weight, axis=1)  # a column with weights in grams\n",
    "filtered_products.loc[:,'price_kg'] = filtered_products.price / filtered_products.weight * 1000   # a column with prices per kg\n",
    "\n",
    "filtered_products.loc[:,'number_of_units'] = filtered_products.apply(extract_number_of_units, axis=1)  # a column with number of units\n",
    "filtered_products.loc[:,'price_unit'] = filtered_products.price / filtered_products.number_of_units   # a column with prices per unit\n",
    "\n",
    "filtered_products.loc[:,'volume'] = filtered_products.apply(extract_volume, axis=1)  # a column with volume in ml\n",
    "filtered_products.loc[:,'price_lit'] = filtered_products.price / filtered_products.volume * 1000   # a column with prices per liter\n",
    "\n",
    "# optional intermediate output\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(filtered_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b0cb5-c194-4e11-b743-e47e06f45650",
   "metadata": {},
   "source": [
    "#### 3.5. Saving the Final Filtered Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c01a4b-d3e3-4342-8843-ef0fab94114c",
   "metadata": {},
   "source": [
    "Finally, the enriched dataset is saved to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0475b-0eae-43d8-8d4d-62324e3fbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_products.to_csv(f'filtered_products-2025-03-03-pyaterochka.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
